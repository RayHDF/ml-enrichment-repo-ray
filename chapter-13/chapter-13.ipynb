{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13 - Loading and Preprocessing Data with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Why would you want to use the Data API?\n",
    "    sebaiknya menggunakan Data API (tf.data) untuk membangun pipeline input yang sangat efisien dan cepat. Ini memungkinkan pemrosesan data dalam jumlah besar secara paralel, memuat data di latar belakang (prefetching) untuk menjaga agar GPU tetap sibuk, dan menyederhanakan manipulasi dataset yang kompleks.\n",
    "\n",
    "2. What are the benefits of splitting a large dataset into multiple files?\n",
    "    Membagi dataset besar menjadi beberapa file memungkinkan shuffling yang lebih baik dengan mencampur urutan file dan isinya. Ini juga memfasilitasi pemuatan dan pemrosesan data secara paralel dari beberapa file sekaligus, yang secara signifikan mempercepat training.\n",
    "\n",
    "3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "    dapat mengetahui pipeline input menjadi bottleneck jika utilisasi GPU rendah selama training. Untuk memperbaikinya, tambahkan .prefetch(tf.data.AUTOTUNE) di akhir pipeline, paralelkan transformasi data menggunakan .map() dengan num_parallel_calls, dan gunakan .cache() jika dataset muat di memori.\n",
    "\n",
    "4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "    Ya, dapat menyimpan data biner apa pun dalam file TFRecord. Format ini pada dasarnya adalah wadah untuk urutan catatan biner, yang bisa berupa serialized protocol buffer, gambar JPEG, atau data biner lainnya.\n",
    "\n",
    "5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "    Menggunakan format tf.train.Example yang standar memastikan kompatibilitas dengan ekosistem TensorFlow yang lebih luas, seperti TF Transform dan TFX. Ini menghilangkan kebutuhan untuk menulis dan mengelola definisi sendiri dan sudah cukup fleksibel untuk sebagian besar kasus penggunaan.\n",
    "\n",
    "6. When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "    Aktifkan kompresi pada TFRecords untuk menghemat ruang disk atau bandwidth jaringan, terutama saat membaca dari penyimpanan jarak jauh. Namun, jangan melakukannya secara sistematis karena proses dekompresi memerlukan waktu CPU tambahan, yang dapat memperlambat pipeline data jika sudah terikat oleh CPU (CPU-bound).\n",
    "\n",
    "7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
    "    Pra-pemrosesan saat menulis file sangat cepat saat training tetapi tidak fleksibel. Pra-pemrosesan dalam pipeline tf.data fleksibel tetapi dapat memperlambat training. Pra-pemrosesan di dalam lapisan model menyederhanakan deployment tetapi menambah beban saat inferensi. TF Transform adalah yang paling kuat karena menggabungkan analisis dataset penuh dengan eksekusi yang efisien, mencegah training-serving skew, tetapi menambah kompleksitas pada alur kerja Anda.\n",
    "\n",
    "8. Name a few common techniques you can use to encode categorical features. What about text?\n",
    "    Untuk fitur kategorikal, teknik umum adalah One-Hot Encoding atau Embeddings, di mana embeddings lebih disukai dalam deep learning karena menangkap hubungan antar kategori. Untuk teks, pendekatan berkisar dari Bag-of-Words dan TF-IDF yang sederhana hingga Word Embeddings dan model Transformer canggih seperti BERT yang menangkap konteks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_13028\\558750724.py:1: unbatch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unbatch()`.\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
    "dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
